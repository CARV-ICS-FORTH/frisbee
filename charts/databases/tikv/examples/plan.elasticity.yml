---
apiVersion: frisbee.dev/v1alpha1
kind: Scenario
metadata:
  name: tikv-elasticity
spec:
  actions:
    # Step 0: bootstrap.
    # For TiKV, we must first create a placementDriver and then add the workers.
    - action: Service
      name: coordinator
      service:
        templateRef: tikv.cluster.placement-driver

    # add a cluster of 3 TiKV instances
    - action: Cluster
      depends: { running: [ coordinator ] }
      name: workers
      cluster:
        templateRef: tikv.cluster.worker
        instances: 3
        inputs:
          - { placementDriver: coordinator }

    # Step 1: populate the cluster with keys
    # We use no throttling to maximize this step and complete it soon.
    - action: Cluster
      name: loaders
      depends: { running: [ coordinator, workers ] }
      cluster:
        templateRef: ycsb.tikv.loader
        inputs:
          - { server: coordinator, recordcount: "1000000", offset: "0", threads: "400" }


    # Step 2: gradually increase the number of TiKV servers
    # We use no throttling to maximize this step and complete it soon.
    - action: Cluster
      depends: { running: [ coordinator ], success: [ loaders ] }
      name: more-workers
      cluster:
        templateRef: tikv.cluster.worker
        instances: 5
        inputs:
          - { placementDriver: coordinator }
        tolerate:
          failedJobs: 5
        schedule:
          cron: "@every 1m"

    # Step 3: gradually decrease the number of TiKV servers
    - action: Cascade
      name: killer
      depends: { running: [ more-workers ] }
      cascade:
        templateRef: system.chaos.pod.kill
        inputs:
          - { target: more-workers-4 }
          - { target: more-workers-7 }
          - { target: more-workers-3 }
        schedule:
          cron: "@every 1m"    

    # Teardown
    - action: Delete
      name: teardown
      depends: { running: [ coordinator, workers, more-workers ], success: [ killer] }
      delete:
        jobs: [ coordinator,  workers, more-workers ]