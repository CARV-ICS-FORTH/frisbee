---
apiVersion: frisbee.dev/v1alpha1
kind: Scenario
metadata:
  name: tikv-elasticity
spec:
  actions:
    # Step 0: bootstrap.
    # For TiKV, we must first create a placementDriver and then add the workers.
    - action: Service
      name: master
      service:
        templateRef: tikv.cluster.placement-driver
        inputs:
          - { cpu: "4", memory: "8Gi" }


    # add a cluster of 3 TiKV instances
    - action: Cluster
      depends: { running: [ master ] }
      name: workers
      cluster:
        templateRef: tikv.cluster.worker
        instances: 3
        inputs:
          - { placementDriver: .service.master.one }


    # Step 1: populate the cluster with keys
    # We use no throttling to maximize this step and complete it soon.
    - action: Cluster
      name: loaders
      depends: { running: [ master, workers ] }
      cluster:
        templateRef: ycsb.tikv.loader
        inputs:
          - { server: .service.master.one, recordcount: "1000000", offset: "0", threads: "400" }


    # Step 2: gradually increase the number of TiKV servers
    # We use no throttling to maximize this step and complete it soon.
    - action: Cluster
      depends: { running: [ master ], success: [ loaders ] }
      name: moreWorkers
      cluster:
        templateRef: tikv.cluster.worker
        instances: 5
        inputs:
          - { placementDriver: .service.master.one }
        tolerate:
          failedJobs: 5
        schedule:
          cron: "@every 1m"

    # Step 3: gradually decrease the number of TiKV servers
    - action: Chaos
      name: killer1
      depends: { running: [ workers, moreWorkers ] }
      chaos:
        templateRef: system.chaos.pod.kill
        inputs:
          - { target: .cluster.moreWorkers.one }

    - action: Chaos
      name: killer2
      depends: { running: [ workers, moreWorkers, killer1 ] }
      chaos:
        templateRef: system.chaos.pod.kill
        inputs:
          - { target: .cluster.moreWorkers.one }

    - action: Chaos
      name: killer3
      depends: { running: [ workers, moreWorkers, killer2 ] }
      chaos:
        templateRef: system.chaos.pod.kill
        inputs:
          - { target: .cluster.moreWorkers.one }

    - action: Chaos
      name: killer4
      depends: { running: [ workers, moreWorkers, killer3 ] }
      chaos:
        templateRef: system.chaos.pod.kill
        inputs:
          - { target: .cluster.moreWorkers.one }

    - action: Chaos
      name: killer5
      depends: { running: [ workers, moreWorkers, killer4 ] }
      chaos:
        templateRef: system.chaos.pod.kill
        inputs:
          - { target: .cluster.moreWorkers.one }