---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.server
spec:
  service:
    containers:
      - name: app
        image: czero/iperf2
        ports:
          - name: listen
            containerPort: 5001
        resources:
          limits:
            cpu: "0.2"
            memory: "500Mi"
        command: [ iperf ]
        args: [ "-s", "-f", "m", "-i", "5" ]


---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.client
spec:
  inputs:
    parameters:
      target: localhost
      duration: "60"
  service:
    containers:
      - name: app
        image: czero/iperf2
        command: [ iperf ]
        args: [ "-c", "{{.inputs.parameters.target}}", "-t", "{{.inputs.parameters.duration}}" ]


---
apiVersion: frisbee.dev/v1alpha1
kind: Scenario
metadata:
  name: advanced-placement
spec:
  actions:
    - action: Service
      name: server
      service:
        templateRef: iperf.server

    - action: Cluster
      name: group-a
      depends: { running: [ server ] }
      cluster:
        templateRef: iperf.client
        instances: 5
        inputs:
          - { target: server, duration: "10" }
          - { target: server, duration: "20" }
          - { target: server, duration: "30" }
        # Place all clients on the same node.
        # Ensure that the node will be different
        # from where the server is running
        placement:
          collocate: true
          conflictsWith: [ server ]

    - action: Cluster
      name: group-b
      depends: { running: [ server ] }
      cluster:
        templateRef: iperf.client
        instances: 5
        inputs:
          - { target: server, duration: "10" }
          - { target: server, duration: "20" }
          - { target: server, duration: "30" }
        # Place all clients on the same node.
        # Ensure that the node will be different
        # from where the server is running
        placement:
          collocate: false
          conflictsWith: [ server, group-a ]


    # When all actions are done, delete looping servers to gracefully exit the experiment
    - action: Delete
      name: teardown
      depends: { running: [ server ], success: [ group-a, group-b ] }
      delete:
        jobs: [ server ]