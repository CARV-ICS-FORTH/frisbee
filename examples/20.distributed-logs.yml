---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.server
spec:
  service:
    decorators:
      telemetry: [system.telemetry.agent]
      resources:
        cpu: "1"
        memory: "48Mi"
    containers:
      - name: app
        image: czero/iperf2
        ports:
          - name: listen
            containerPort: 5001
        command:
          - /bin/sh
          - -c
          - |         
              set -eum
              cut -d ' ' -f 4 /proc/self/stat > /dev/shm/app # Sidecar: use it for entering the cgroup
              
              iperf -s -f m -i 5

---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.client
spec:
  inputs:
    parameters:
      target: localhost
  service:
    decorators:
      telemetry:
        - system.telemetry.agent
      resources:
        cpu: "0.2"
        memory: "48Mi"

    containers:
      - name: app
        image: czero/iperf2
        command:
          - /bin/sh
          - -c
          - |         
              set -eum
              cut -d ' ' -f 4 /proc/self/stat > /dev/shm/app
    
              iperf -c {{.Inputs.Parameters.target}} -t 120 -i 5  >> /testdata/my_logs

---
# In order to have shared logs, we must first create a network volume.
# This volume will then be mounted across the various containers.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: logs
spec:
  storageClassName: network-volume
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi


---
apiVersion: frisbee.dev/v1alpha1
kind: Scenario
metadata:
  name: distributed-logs
spec:
  testData:
    volume: { claimName: logs}

  actions:
    - action: Service
      name: server
      service:
        templateRef: iperf.server

    - action: Cluster
      name: clients
      depends: { running: [ server ] }
      cluster:
        templateRef: iperf.client
        instances: 5
        inputs:
          - { target: server  }
        schedule:
          cron: "@every 1m"


    # When all actions are done, delete looping servers to gracefully exit the experiment
    - action: Delete
      name: teardown
      depends: { running: [ server], success: [clients] }
      delete:
        jobs: [ server ]