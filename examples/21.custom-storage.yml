---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.server
spec:
  service:
    decorators:
      telemetry:
        - system.telemetry.agent
      resources:
        cpu: "1"
        memory: "48Mi"
    containers:
      - name: app
        image: czero/iperf2
        ports:
          - name: listen
            containerPort: 5001
        command:
          - /bin/sh
          - -c
          - |         
              set -eum
              cut -d ' ' -f 4 /proc/self/stat > /dev/shm/app # Sidecar: use it for entering the cgroup
              
              iperf -s -f m -i 5

---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.client
spec:
  inputs:
    parameters:
      target: localhost
      logClaimName: ""
  service:
    decorators:
      telemetry:
        - system.telemetry.agent
      resources:
        cpu: "0.2"
        memory: "48Mi"

    volumes: # Need to declare the volume here
      - name: logvolume
        persistentVolumeClaim:
          claimName: "{{.Inputs.Parameters.logClaimName}}"

    containers:
      - name: app
        image: czero/iperf2
        volumeMounts: # Need to mount the volume here
          - name: logvolume
            mountPath: /logs
        command:
          - /bin/sh
          - -c
          - |         
              set -eum
              cut -d ' ' -f 4 /proc/self/stat > /dev/shm/app
    
              iperf -c {{.Inputs.Parameters.target}} -t 360 -i 5  >> /logs/${HOSTNAME}

---
# In order to have shared logs, we must first create a network volume.
# This volume will then be mounted across the various containers.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: logs
spec:
  storageClassName: network-volume
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi


---
apiVersion: frisbee.dev/v1alpha1
kind: Scenario
metadata:
  name: distributed-logs
spec:
  actions:
    # Step 0. Enable the log viewer
    - action: Service
      name: logviewer
      service:
        templateRef: system.telemetry.logviewer.template
        inputs:
          - { logClaimName: logs }

    - action: Service
      depends: { running: [ logviewer ] }
      name: server
      service:
        templateRef: iperf.server

    - action: Cluster
      name: clients
      depends: { running: [ server ] }
      cluster:
        templateRef: iperf.client
        instances: 5
        inputs:
          - { target: server, logClaimName: logs  }
        schedule:
          cron: "@every 1m"


    # When all actions are done, delete looping servers to gracefully exit the experiment
    - action: Delete
      name: teardown
      depends: { running: [ server, logviewer], success: [clients] } # Notice: We want the logviewer to remain running
      delete:
        jobs: [ server ]