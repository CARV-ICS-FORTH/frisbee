---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.server
spec:
  service:
    decorators:
      telemetry: [ system.telemetry.agent ]
    containers:
      - name: app
        image: czero/iperf2
        ports:
          - name: listen
            containerPort: 5001
        resources:
          limits:
            cpu: "0.2"
            memory: "500Mi"
        command:
          - /bin/sh
          - -c
          - |
            set -eum
            cut -d ' ' -f 4 /proc/self/stat > /dev/shm/app # Sidecar: use it for entering the cgroup
            
            iperf -s -f m -i 5

---
apiVersion: frisbee.dev/v1alpha1
kind: Template
metadata:
  name: iperf.client
spec:
  inputs:
    parameters:
      target: localhost
  service:
    decorators:
      telemetry:
        - system.telemetry.agent

    containers:
      - name: app
        image: czero/iperf2
        resources:
          limits:
            cpu: "0.2"
            memory: "500Mi"
        command:
          - /bin/sh
          - -c
          - |
            set -eum
            cut -d ' ' -f 4 /proc/self/stat > /dev/shm/app
            
            iperf -c {{.inputs.parameters.target}} -t 120 -i 5  >> /testdata/my_logs

---
# In order to have shared logs, we must first create a network volume.
# This volume will then be mounted across the various containers.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: logs
spec:
  storageClassName: network-volume
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi


---
apiVersion: frisbee.dev/v1alpha1
kind: Scenario
metadata:
  name: testdata
spec:
  testData:
    volume: { claimName: logs }

  actions:
    - action: Service
      name: server
      service:
        templateRef: iperf.server

    - action: Cluster
      name: clients
      depends: { running: [ server ] }
      cluster:
        templateRef: iperf.client
        instances: 5
        inputs:
          - { target: server }
        schedule:
          cron: "@every 1m"


    # When all actions are done, delete looping servers to gracefully exit the experiment
    - action: Delete
      name: teardown
      depends: { running: [ server ], success: [ clients ] }
      delete:
        jobs: [ server ]