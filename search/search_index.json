{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"chart-developer/","text":"Guide for the Frisbee Chart Developers What is a Helm Chart ? Lint Charts Working with MicroK8s\u2019 built-in registry Run a Test Debugging an Installation Debug a Test Write a test Change the Code Run a test Step 1: Install Dependencies Step 2: Update Helm repo Step 3: This step will install the following components: Step 4: Install the testing components Step 5: Run the Test Plan Observe a Testplan Kubernetes Dashboard Controller Logs Grafana Dashboard & Alerts HELM Baseline YCSB FIO Stress Scaleout Elasticity Chaos Guide for the Frisbee Plan Developers Periodically kill some nodes. Do not set dependencies on cascades This is a guide for those who wish to contribute new Charts in Frisbee. Because there is an overlap, we advise you to have a look at the Guide for Code Developers first. What is a Helm Chart ? Helm is a package manager for Kubernetes. Helm uses a packaging format called charts . A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on. The best way to start is by reading the official Helm documentation . Control how to visualize the services. // DrawAs hints how to mark points on the Grafana dashboard. DrawAs string = \"frisbee.io/draw/\" // DrawAsPoint will mark the creation and deletion of a service as distinct events. DrawAsPoint string = \"pointInTime\" // DrawAsRegion will draw a region starting from the creation of a service and ending to the deletion of the service. DrawAsRegion string = \"timeRegion\" Lint Charts yamllint ./platform/Chart.yaml docker run quay.io/helmpack/chart-testing:lates ct lint --target-branch=main --check-version-increment=false Working with MicroK8s\u2019 built-in registry # Install the registry microk8s enable registry # To upload images we have to tag them with localhost:32000/your-image before pushing them: docker build . -t localhost:32000/mynginx:registry # Now that the image is tagged correctly, it can be pushed to the registry: docker push localhost:32000/mynginx Pushing to this insecure registry may fail in some versions of Docker unless the daemon is explicitly configured to trust this registry. To address this we need to edit /etc/docker/daemon.json and add: { \"insecure-registries\": [ \"localhost:32000\" ] } The new configuration should be loaded with a Docker daemon restart: sudo systemctl restart docker Source: https://microk8s.io/docs/registry-built-in Run a Test The easiest way to begin with is by have a look at the examples. Let's assume you are interested in testing TiKV. >> cd examples/tikv/ >> ls templates plan.baseline.yml plan.elasticity.yml plan.saturation.yml plan.scaleout.yml You will some plan.*.yml files, and a sub-directory called templates . Templates: are libraries of frequently-used specifications that are reusable throughout the testing plan. Plans: are lists of actions that define what will happen throughout the test. In general, a plan may dependent on numerous templates, and the templates depend on other templates. To run the test will all the dependencies satisfied: # Create a dedicated Frisbee name >> kubectl create namespace karvdash-fnikol # Run the test >> kubectl -n karvdash-fnikol apply -f ../core/observability/ -f ../core/ycsb/ -f ./templates/ -f templates/telemetry/ -f plan.baseline.yml For TikV, the dependencies are: ./templates/ : TiKV servers and clients ./templates/telemetry : Telemetry for TiKV servers (TiKV-specific metrics) examples/templates/core/observability : Telemetry for TiKV containers (system-wise metrics) examples/templates/core/ycsb : Telemetry for TiKV clients (YCSB-specific metrics) BEWARE: 1) flag -f does not work recursively. You must explicitly declare the telemetry directory. 2) If you modify a template, you must re-apply it Debugging an Installation Error: UPGRADE FAILED: unable to recognize \"\": no matches for kind \"Template\" in version \"frisbee.io/v1alpha1\" In the next step, you should validate that CRDs are successfully installed. # Validate the CRDs are properly installed >> kubectl get crds | grep frisbee.io chaos.frisbee.io 2021-12-17T12:30:06Z clusters.frisbee.io 2021-12-17T12:30:06Z services.frisbee.io 2021-12-17T12:30:07Z telemetries.frisbee.io 2021-12-17T12:30:07Z workflows.frisbee.io 2021-12-17T12:30:07Z Debug a Test At this point, the workflow is installed. You can go to the controller's terminal and see some progress. If anything fails, you will it see it from there. $ sudo curl -s https://raw.githubusercontent.com/ncarlier/webhookd/master/install.sh | bash Write a test helm install --dry-run --debug --dependency-update ./ ../observability/ https://github.com/helm/chartmuseum https://medium.com/@maanadev/how-set-up-a-helm-chart-repository-using-apache-web-server-670ffe0e63c7 Deploy as helm chart helm repo index ./ --url https://carv-ics-forth.github.io/frisbee Change the Code The easiest way to begin with is by have a look at the examples. It consists of two sub-directories: * **Templates:** are libraries of frequently-used specifications that are reusable throughout the testing plan. * **Testplans:** are lists of actions that define what will happen throughout the test. We will use the `examples/testplans/3.failover.yml` as a reference. This plans uses the following templates: * `examples/templates/core/sysmon.yml` * `examples/templates/redis/redis.cluster.yml` * `examples/templates/ycsb/redis.client.yml` Because these templates are deployed as Kubernetes resources, they are references by name rather than by the relative path. This is why we need to have them installed before running the experiment. (for installation instructions check [here](docs/singlenode-deployment.md).) ```yaml # Standard Kubernetes boilerplate apiVersion: frisbee.io/v1alpha1 kind: TestPlan metadata: name: redis-failover spec: # Here we specify the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each action. actions: # Service creates an instance of a Redis Master # To create the instance we use the redis.single.master with the default parameters. - action: Service name: master service: templateRef: redis.single.master # This action is same as before, with two additions. # 1. The `depends' keyword ensure that the action will be executed only after the `master' action # has reached a Running state. # 2. The `inputs' keyword initialized the instance with custom parameters. - action: Service name: slave depends: { running: [ master ] } service: templateRef: redis.single.slave inputs: - { master: .service.master.one } # The sentinel is Redis failover manager. Notice that we can have multiple dependencies. - action: Service name: sentinel depends: { running: [ master, slave ] } service: templateRef: redis.single.sentinel inputs: - { master: .service.master.one } # Cluster creates a list of services that run a shared context. # In this case, we create a cluster of YCSB loaders to populate the master with keys. - action: Cluster name: \"loaders\" depends: { running: [ master ] } cluster: templateRef: ycsb.redis.loader inputs: - { server: .service.master.one, recordcount: \"100000000\", offset: \"0\" } - { server: .service.master.one, recordcount: \"100000000\", offset: \"100000000\" } - { server: .service.master.one, recordcount: \"100000000\", offset: \"200000000\" } # While the loaders are running, we inject a network partition fault to the master node. # The \"after\" dependency adds a delay so to have some keys before injecting the fault. # The fault is automatically retracted after 2 minutes. - action: Chaos name: partition0 depends: { running: [ loaders ], after: \"3m\" } chaos: type: partition partition: selector: macro: .service.master.one duration: \"2m\" # Here we repeat the partition, a few minutes after the previous fault has been recovered. - action: Chaos name: partition1 depends: { running: [ master, slave ], success: [ partition0 ], after: \"6m\" } chaos: type: partition partition: selector: { macro: .service.master.one } duration: \"1m\" # Here we declare the Grafana dashboards that Workflow will make use of. withTelemetry: importDashboards: [ \"system.telemetry.agent\", \"ycsb.telemetry.client\", \"redis.telemetry.server\" ] ``` # Run the experiment Firstly, you'll need a Kubernetes deployment and `kubectl` set-up * For a single-node deployment click [here](docs/singlenode-deployment.md). * For a multi-node deployment click [here](docs/cluster-deployment.md). In this walk-through, we assume you have followed the instructions for the single-node deployment. In one terminal, run the Frisbee controller. If you want to run the webhooks locally, you\u2019ll have to generate certificates for serving the webhooks, and place them in the right directory (/tmp/k8s-webhook-server/serving-certs/tls.{crt,key}, by default). _If you\u2019re not running a local API server, you\u2019ll also need to figure out how to proxy traffic from the remote cluster to your local webhook server. For this reason, we generally recommend disabling webhooks when doing your local code-run-test cycle, as we do below._ ```bash # Run the Frisbee controller >> make run ENABLE_WEBHOOKS=false ``` We can use the controller's output to reason about the experiments transition. On the other terminal, you can issue requests. ```bash # Create a dedicated Frisbee name >> kubectl create namespace frisbee # Run a testplan (from Frisbee directory) >> kubectl -n frisbee apply -f examples/testplans/3.failover.yml workflow.frisbee.io/redis-failover created # Confirm that the workflow is running. >> kubectl -n frisbee get pods NAME READY STATUS RESTARTS AGE prometheus 1/1 Running 0 12m grafana 1/1 Running 0 12m master 3/3 Running 0 12m loaders-0 3/3 Running 0 11m slave 3/3 Running 0 11m sentinel 1/1 Running 0 11m # Wait until the test oracle is triggered. >> kubectl -n frisbee wait --for=condition=oracle workflows.frisbee.io/redis-failover ... ``` ## How can I understand what happened ? One way, is to access the workflow's description ```bash >> kubectl -n frisbee describe workflows.frisbee.io/validate-local ``` But why bother if you can access Grafana directly ? # Frisbee in a Nutshell This tutorial introduces the basic functionalities of Frisbee: - **Write tests:** for stressing complex topologies and dynamic operating conditions. - **Run tests:** provides seamless scaling from a single workstation to hundreds of machines. - **Debug tests:** through extensive monitoring and comprehensive dashboards. For the rest of this tutorial we will use the Frisbee package of TiKV key/value store. #### Frisbee Installation Before anything else, we need to install the Frisbee platform and the Frisbee packages for testing. ``` ``` Then you have to go install the Frisbee system. ```bash >> cd charts/frisbee/charts/frisbee >> helm install frisbee ./ --dependency-update ``` You will see a YAML output that describe the components to be installed. #### Run the controller After the Frisbee CRDs and their dependencies are installed, you can start running the Frisbee controller. From the project's directory run ```bash >> make run ``` #### Modify an examples test Perhaps the best way is to modify an existing test. We use the `iperf` benchmark as a reference. From another terminal (do not close the controller), go to ```bash >> cd charts/iperf/ ``` You will see the following files: * **templates**: libraries of frequently-used specifications that are reusable throughout the testing plan. * **plans**: lists of actions that define what will happen throughout the test. * **dashboards**: application-specific dashboards for Grafana Because templates are used by the plans, we must have them installed before the running the tests. ```bash >> helm install iperf ./ --dependency-update ``` Then, run the test to become familiar with the procedure. ```bash >> kubectl apply -f plans/plan.validate-network.yml ``` If everything works fine, you will see the logs flowing in the **controller**. Then you will get a message like > Error: found in Chart.yaml, but missing in charts/ directory: chaos-mesh, openebs This is because the Frisbee installation are not yet downloaded. To get them automatically run the previous command with`--dependency-update` flag. Also, remove the `--dry-run` run to execute the actual installation. ```bash >> helm install frisbee ./ --dependency-update ``` Run the controller == Operatator-SDK === Create a new Controller * operator-sdk create api --group frisbee --version v1alpha1 --kind MyNewController --resource --controller * operator-sdk create webhook --group frisbee --version v1alpha1 --kind MyNewController --defaulting --programmatic-validation docker save frisbee:latest -o image.tar == Notes == 1) Cadvisor does not support for NFS mounts. 2) Check how we can use block devices Run a test Step 1: Install Dependencies Make sure that kubectl and are installed on your system, and that you have access to a Kubernetes installation. Local Installation If you want a local installation you can use Remote Installation : Set ~/.kube/config appropriately, and create tunnel for sending requests to Kubernetes API. # Create tunnel for sending requests to Kubernetes API. >> ssh -L 6443:192.168.1.213:6443 [USER@]SSH_SERVER Step 2: Update Helm repo Step 3: This step will install the following components: Frisbee CRDS Frisbee Controller Frisbee Dependency stack (e.g, Chaos toolkits, dynamic volume provisioning, observability stack) Ingress for making the observability stack accessible from outside the Kubernetes By default the platform sets the Ingress to localhost . If you use a non-local cluster, you can these the ingress via the global.ingress flag. # Install the platform with non-local ingress >> helm upgrade --install --wait my-frisbee frisbee/platform --set global.ingress=platform.science-hangar.eu Step 4: Install the testing components Step 5: Run the Test Plan This url points to : https://raw.githubusercontent.com/CARV-ICS-FORTH/frisbee/main/charts/tikv/examples/plan.baseline.yml # Create a plan >> curl -sSL https://tinyurl.com/t3xrtmny | kubectl -f - apply Observe a Testplan Kubernetes Dashboard If you use a microk8s installation of Kubernetes, then the procedure is slightly different. # Deploy the dashboard >> microk8s dashboard-proxy # Start the dashboard >> microk8s dashboard-proxy # Now access Dashboard at: https://localhost:10443 Controller Logs The logs of the controller are accessible by the terminal on which the controller is running. Grafana Dashboard & Alerts Grafana is a multi-platform open source analytics and interactive visualization web application. To access it, use the format http://grafana.${INGRESS} where Ingress is the value you defined in step 3. For example, # Access Grafana via your browser http://grafana.platform.science-hangar.eu Optionally, validate that everything works. # Deploy a hello world >> kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4 deployment.apps/hello-node created # Verify that a hell-node deployment exists >> kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-node 1/1 1 1 36s # Delete the deployment >> kubectl delete deployments hello-node deployment.apps \"hello-node\" deleted This step will install the Frisbee CRDs and all the necessary tools. # Update Helm repo >> helm repo add frisbee https://carv-ics-forth.github.io/frisbee/charts # Install the platform with local ingress >> helm upgrade --install --wait my-frisbee frisbee/platform KUBECONFIG=\"/home/fnikol/.kube/config.evolve.admin\" make run ahelm upgrade --install --wait my-frisbee ./charts/platform/ --debug --set operator.enabled=false -f ./charts/platform/values-baremetal.yaml ehelm upgrade --install --wait my-system ./charts/system --debug Cool hacks: https://github.com/antrea-io/antrea/tree/main/hack HELM Create a public Helm chart repository with GitHub Pages What is Helm and a Helm chart repository Helm, as per official claim, is \u201cThe package manager for Kubernetes\u201d. Indeed Helm really helps handling application deployments on Kubernetes, not only because it simplifies the application release phase but also because Helm makes possible to easily manage variables in the Kubernetes manifest YAML files. Once the charts are ready and you need to share them, the easiest way to do so is by uploading them to a chart repository. A Helm chart repository is where we host and share Helm packages and any HTTP server will do. Unluckily Helm does not include natively a tool for uploading charts to a remote chart repository server (Helm can serve the local repository via $ helm serve though). We\u2019ll take advantage of GitHub Pages for the purpose to share our charts. What is GitHub Pages GitHub Pages is a static site hosting service provided to you by GitHub, designed to host your pages directly from a GitHub repository. GitHub Pages is a perfect match for our purpose, since everything we need is to host a single index.yaml file along with a bunch of .tgz files. Why not hosting all this stuff in your own web server? A managed service helps to reduce your operational overhead and risk (think to monitoring, patch management, security, backup services\u2026) so you can focus on code and in what makes your business relevant for your customers. Useful links: https://medium.com/@mattiaperi/create-a-public-helm-chart-repository-with-github-pages-49b180dbb417 https://github.com/technosophos/tscharts A Test Plan documents the strategy that will be used to verify a specific test for a software product. The plan typically contains a detailed understanding of the eventual workflow that models the deployment scenario for a software product, the test strategy, the resources required to perform testing, and the Key Performance Indicators required to ensure that a product or system meets its design specifications and other requirements. By codifying the test plan in a YAML-based syntax, Frisbee carriers three main benefits to teams: Help people outside the test team such as developers, business managers, customers understand the details of testing. Test Plan guides our thinking. It is like a rule book, which needs to be followed. Important aspects like test estimation, test scope, Test Strategy are documented in Test Plan, so it can be reviewed by Management Team and re-used for other projects. A test plan may include a strategy for one or more of the following: Baseline: to be performed during the development or approval stages of the product, typically on a small sample of units. Stress: to be performed during preparation or assembly of the product, in an ongoing manner for purposes of performance verification and quality control. Baseline A baseline is a fixed point of reference that is used for comparison purposes . YCSB Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs . It is often used to compare relative performance of NoSQL database management systems. All six workloads have a data set which is similar. Workloads D and E insert records during the test run. Thus, to keep the database size consistent, we apply the following sequence: Bootstrap the database. Load the database, using workload A's parameter file (workloads/workloada) and the \"-load\" switch to the client. Run workload A (using workloads/workloada and \"-t\") for a variety of throughputs. Run workload B (using workloads/workloadb and \"-t\") for a variety of throughputs. Run workload C (using workloads/workloadc and \"-t\") for a variety of throughputs. Run workload F (using workloads/workloadf and \"-t\") for a variety of throughputs. Run workload D (using workloads/workloadd and \"-t\") for a variety of throughputs. This workload inserts records, increasing the size of the database. Delete the data in the database. Otherwise, the remaining data of the cluster might affect the results of the following workload. For the deletion, instead of destroying the cluster, we destroy and recreate the cluster. Reload the database, using workload E's parameter file (workloads/workloade) and the \"-load switch to the client. Run workload E (using workloads/workloadd and \"-t\") for a variety of throughputs. This workload inserts records, increasing the size of the database. In general, these steps remain the same for the various databases. The difference is how we bootstrap each database. FIO To benchmark persistent disk performance, use FIO instead of other disk benchmarking tools such as dd . Fio spawns a number of threads or processes doing a particular type of I/O action as specified by the user. Stress Scaleout Elasticity Chaos Guide for the Frisbee Plan Developers Spurious Alert may be risen if the expr evaluation frequency is less than the scheduled interval. In this case, Grafana faces an idle period, and raises a NoData Alert. The controller ignores such messages. Periodically kill some nodes. action: Cascade name: killer depends: { running: [ clients ] } cascade: templateRef: system.chaos.pod.kill instances: 3 inputs: { target: .cluster.clients.one } This can be wrong because Frisbee selects a single client -- and will be used 3 times, without error. Instead, we must use as many inputs as the number of instances -- or omit instances. In general, because when you use one input for multiple instances. Macros select only Running objects Do not set dependencies on cascades This is because Kills are always running -- therefore cascades that involve kill actions are always running","title":"Chart developer"},{"location":"chart-developer/#guide-for-the-frisbee-chart-developers","text":"What is a Helm Chart ? Lint Charts Working with MicroK8s\u2019 built-in registry Run a Test Debugging an Installation Debug a Test Write a test Change the Code Run a test Step 1: Install Dependencies Step 2: Update Helm repo Step 3: This step will install the following components: Step 4: Install the testing components Step 5: Run the Test Plan Observe a Testplan Kubernetes Dashboard Controller Logs Grafana Dashboard & Alerts HELM Baseline YCSB FIO Stress Scaleout Elasticity Chaos Guide for the Frisbee Plan Developers Periodically kill some nodes. Do not set dependencies on cascades This is a guide for those who wish to contribute new Charts in Frisbee. Because there is an overlap, we advise you to have a look at the Guide for Code Developers first.","title":"Guide for the Frisbee Chart Developers"},{"location":"chart-developer/#what-is-a-helm-chart","text":"Helm is a package manager for Kubernetes. Helm uses a packaging format called charts . A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on. The best way to start is by reading the official Helm documentation . Control how to visualize the services. // DrawAs hints how to mark points on the Grafana dashboard. DrawAs string = \"frisbee.io/draw/\" // DrawAsPoint will mark the creation and deletion of a service as distinct events. DrawAsPoint string = \"pointInTime\" // DrawAsRegion will draw a region starting from the creation of a service and ending to the deletion of the service. DrawAsRegion string = \"timeRegion\"","title":"What is a Helm Chart ?"},{"location":"chart-developer/#lint-charts","text":"yamllint ./platform/Chart.yaml docker run quay.io/helmpack/chart-testing:lates ct lint --target-branch=main --check-version-increment=false","title":"Lint Charts"},{"location":"chart-developer/#working-with-microk8s-built-in-registry","text":"# Install the registry microk8s enable registry # To upload images we have to tag them with localhost:32000/your-image before pushing them: docker build . -t localhost:32000/mynginx:registry # Now that the image is tagged correctly, it can be pushed to the registry: docker push localhost:32000/mynginx Pushing to this insecure registry may fail in some versions of Docker unless the daemon is explicitly configured to trust this registry. To address this we need to edit /etc/docker/daemon.json and add: { \"insecure-registries\": [ \"localhost:32000\" ] } The new configuration should be loaded with a Docker daemon restart: sudo systemctl restart docker Source: https://microk8s.io/docs/registry-built-in","title":"Working with MicroK8s\u2019 built-in registry"},{"location":"chart-developer/#run-a-test","text":"The easiest way to begin with is by have a look at the examples. Let's assume you are interested in testing TiKV. >> cd examples/tikv/ >> ls templates plan.baseline.yml plan.elasticity.yml plan.saturation.yml plan.scaleout.yml You will some plan.*.yml files, and a sub-directory called templates . Templates: are libraries of frequently-used specifications that are reusable throughout the testing plan. Plans: are lists of actions that define what will happen throughout the test. In general, a plan may dependent on numerous templates, and the templates depend on other templates. To run the test will all the dependencies satisfied: # Create a dedicated Frisbee name >> kubectl create namespace karvdash-fnikol # Run the test >> kubectl -n karvdash-fnikol apply -f ../core/observability/ -f ../core/ycsb/ -f ./templates/ -f templates/telemetry/ -f plan.baseline.yml For TikV, the dependencies are: ./templates/ : TiKV servers and clients ./templates/telemetry : Telemetry for TiKV servers (TiKV-specific metrics) examples/templates/core/observability : Telemetry for TiKV containers (system-wise metrics) examples/templates/core/ycsb : Telemetry for TiKV clients (YCSB-specific metrics) BEWARE: 1) flag -f does not work recursively. You must explicitly declare the telemetry directory. 2) If you modify a template, you must re-apply it","title":"Run a Test"},{"location":"chart-developer/#debugging-an-installation","text":"Error: UPGRADE FAILED: unable to recognize \"\": no matches for kind \"Template\" in version \"frisbee.io/v1alpha1\" In the next step, you should validate that CRDs are successfully installed. # Validate the CRDs are properly installed >> kubectl get crds | grep frisbee.io chaos.frisbee.io 2021-12-17T12:30:06Z clusters.frisbee.io 2021-12-17T12:30:06Z services.frisbee.io 2021-12-17T12:30:07Z telemetries.frisbee.io 2021-12-17T12:30:07Z workflows.frisbee.io 2021-12-17T12:30:07Z","title":"Debugging an Installation"},{"location":"chart-developer/#debug-a-test","text":"At this point, the workflow is installed. You can go to the controller's terminal and see some progress. If anything fails, you will it see it from there. $ sudo curl -s https://raw.githubusercontent.com/ncarlier/webhookd/master/install.sh | bash","title":"Debug a Test"},{"location":"chart-developer/#write-a-test","text":"helm install --dry-run --debug --dependency-update ./ ../observability/ https://github.com/helm/chartmuseum https://medium.com/@maanadev/how-set-up-a-helm-chart-repository-using-apache-web-server-670ffe0e63c7 Deploy as helm chart helm repo index ./ --url https://carv-ics-forth.github.io/frisbee","title":"Write a test"},{"location":"chart-developer/#change-the-code","text":"The easiest way to begin with is by have a look at the examples. It consists of two sub-directories: * **Templates:** are libraries of frequently-used specifications that are reusable throughout the testing plan. * **Testplans:** are lists of actions that define what will happen throughout the test. We will use the `examples/testplans/3.failover.yml` as a reference. This plans uses the following templates: * `examples/templates/core/sysmon.yml` * `examples/templates/redis/redis.cluster.yml` * `examples/templates/ycsb/redis.client.yml` Because these templates are deployed as Kubernetes resources, they are references by name rather than by the relative path. This is why we need to have them installed before running the experiment. (for installation instructions check [here](docs/singlenode-deployment.md).) ```yaml # Standard Kubernetes boilerplate apiVersion: frisbee.io/v1alpha1 kind: TestPlan metadata: name: redis-failover spec: # Here we specify the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each action. actions: # Service creates an instance of a Redis Master # To create the instance we use the redis.single.master with the default parameters. - action: Service name: master service: templateRef: redis.single.master # This action is same as before, with two additions. # 1. The `depends' keyword ensure that the action will be executed only after the `master' action # has reached a Running state. # 2. The `inputs' keyword initialized the instance with custom parameters. - action: Service name: slave depends: { running: [ master ] } service: templateRef: redis.single.slave inputs: - { master: .service.master.one } # The sentinel is Redis failover manager. Notice that we can have multiple dependencies. - action: Service name: sentinel depends: { running: [ master, slave ] } service: templateRef: redis.single.sentinel inputs: - { master: .service.master.one } # Cluster creates a list of services that run a shared context. # In this case, we create a cluster of YCSB loaders to populate the master with keys. - action: Cluster name: \"loaders\" depends: { running: [ master ] } cluster: templateRef: ycsb.redis.loader inputs: - { server: .service.master.one, recordcount: \"100000000\", offset: \"0\" } - { server: .service.master.one, recordcount: \"100000000\", offset: \"100000000\" } - { server: .service.master.one, recordcount: \"100000000\", offset: \"200000000\" } # While the loaders are running, we inject a network partition fault to the master node. # The \"after\" dependency adds a delay so to have some keys before injecting the fault. # The fault is automatically retracted after 2 minutes. - action: Chaos name: partition0 depends: { running: [ loaders ], after: \"3m\" } chaos: type: partition partition: selector: macro: .service.master.one duration: \"2m\" # Here we repeat the partition, a few minutes after the previous fault has been recovered. - action: Chaos name: partition1 depends: { running: [ master, slave ], success: [ partition0 ], after: \"6m\" } chaos: type: partition partition: selector: { macro: .service.master.one } duration: \"1m\" # Here we declare the Grafana dashboards that Workflow will make use of. withTelemetry: importDashboards: [ \"system.telemetry.agent\", \"ycsb.telemetry.client\", \"redis.telemetry.server\" ] ``` # Run the experiment Firstly, you'll need a Kubernetes deployment and `kubectl` set-up * For a single-node deployment click [here](docs/singlenode-deployment.md). * For a multi-node deployment click [here](docs/cluster-deployment.md). In this walk-through, we assume you have followed the instructions for the single-node deployment. In one terminal, run the Frisbee controller. If you want to run the webhooks locally, you\u2019ll have to generate certificates for serving the webhooks, and place them in the right directory (/tmp/k8s-webhook-server/serving-certs/tls.{crt,key}, by default). _If you\u2019re not running a local API server, you\u2019ll also need to figure out how to proxy traffic from the remote cluster to your local webhook server. For this reason, we generally recommend disabling webhooks when doing your local code-run-test cycle, as we do below._ ```bash # Run the Frisbee controller >> make run ENABLE_WEBHOOKS=false ``` We can use the controller's output to reason about the experiments transition. On the other terminal, you can issue requests. ```bash # Create a dedicated Frisbee name >> kubectl create namespace frisbee # Run a testplan (from Frisbee directory) >> kubectl -n frisbee apply -f examples/testplans/3.failover.yml workflow.frisbee.io/redis-failover created # Confirm that the workflow is running. >> kubectl -n frisbee get pods NAME READY STATUS RESTARTS AGE prometheus 1/1 Running 0 12m grafana 1/1 Running 0 12m master 3/3 Running 0 12m loaders-0 3/3 Running 0 11m slave 3/3 Running 0 11m sentinel 1/1 Running 0 11m # Wait until the test oracle is triggered. >> kubectl -n frisbee wait --for=condition=oracle workflows.frisbee.io/redis-failover ... ``` ## How can I understand what happened ? One way, is to access the workflow's description ```bash >> kubectl -n frisbee describe workflows.frisbee.io/validate-local ``` But why bother if you can access Grafana directly ? # Frisbee in a Nutshell This tutorial introduces the basic functionalities of Frisbee: - **Write tests:** for stressing complex topologies and dynamic operating conditions. - **Run tests:** provides seamless scaling from a single workstation to hundreds of machines. - **Debug tests:** through extensive monitoring and comprehensive dashboards. For the rest of this tutorial we will use the Frisbee package of TiKV key/value store. #### Frisbee Installation Before anything else, we need to install the Frisbee platform and the Frisbee packages for testing. ``` ``` Then you have to go install the Frisbee system. ```bash >> cd charts/frisbee/charts/frisbee >> helm install frisbee ./ --dependency-update ``` You will see a YAML output that describe the components to be installed. #### Run the controller After the Frisbee CRDs and their dependencies are installed, you can start running the Frisbee controller. From the project's directory run ```bash >> make run ``` #### Modify an examples test Perhaps the best way is to modify an existing test. We use the `iperf` benchmark as a reference. From another terminal (do not close the controller), go to ```bash >> cd charts/iperf/ ``` You will see the following files: * **templates**: libraries of frequently-used specifications that are reusable throughout the testing plan. * **plans**: lists of actions that define what will happen throughout the test. * **dashboards**: application-specific dashboards for Grafana Because templates are used by the plans, we must have them installed before the running the tests. ```bash >> helm install iperf ./ --dependency-update ``` Then, run the test to become familiar with the procedure. ```bash >> kubectl apply -f plans/plan.validate-network.yml ``` If everything works fine, you will see the logs flowing in the **controller**. Then you will get a message like > Error: found in Chart.yaml, but missing in charts/ directory: chaos-mesh, openebs This is because the Frisbee installation are not yet downloaded. To get them automatically run the previous command with`--dependency-update` flag. Also, remove the `--dry-run` run to execute the actual installation. ```bash >> helm install frisbee ./ --dependency-update ``` Run the controller == Operatator-SDK === Create a new Controller * operator-sdk create api --group frisbee --version v1alpha1 --kind MyNewController --resource --controller * operator-sdk create webhook --group frisbee --version v1alpha1 --kind MyNewController --defaulting --programmatic-validation docker save frisbee:latest -o image.tar == Notes == 1) Cadvisor does not support for NFS mounts. 2) Check how we can use block devices","title":"Change the Code"},{"location":"chart-developer/#run-a-test_1","text":"","title":"Run a test"},{"location":"chart-developer/#step-1-install-dependencies","text":"Make sure that kubectl and are installed on your system, and that you have access to a Kubernetes installation. Local Installation If you want a local installation you can use Remote Installation : Set ~/.kube/config appropriately, and create tunnel for sending requests to Kubernetes API. # Create tunnel for sending requests to Kubernetes API. >> ssh -L 6443:192.168.1.213:6443 [USER@]SSH_SERVER","title":"Step 1:  Install Dependencies"},{"location":"chart-developer/#step-2-update-helm-repo","text":"","title":"Step 2: Update Helm repo"},{"location":"chart-developer/#step-3","text":"","title":"Step 3:"},{"location":"chart-developer/#this-step-will-install-the-following-components","text":"Frisbee CRDS Frisbee Controller Frisbee Dependency stack (e.g, Chaos toolkits, dynamic volume provisioning, observability stack) Ingress for making the observability stack accessible from outside the Kubernetes By default the platform sets the Ingress to localhost . If you use a non-local cluster, you can these the ingress via the global.ingress flag. # Install the platform with non-local ingress >> helm upgrade --install --wait my-frisbee frisbee/platform --set global.ingress=platform.science-hangar.eu","title":"This step will install the following components:"},{"location":"chart-developer/#step-4-install-the-testing-components","text":"","title":"Step 4:  Install the testing components"},{"location":"chart-developer/#step-5-run-the-test-plan","text":"This url points to : https://raw.githubusercontent.com/CARV-ICS-FORTH/frisbee/main/charts/tikv/examples/plan.baseline.yml # Create a plan >> curl -sSL https://tinyurl.com/t3xrtmny | kubectl -f - apply","title":"Step 5: Run the Test Plan"},{"location":"chart-developer/#observe-a-testplan","text":"","title":"Observe a Testplan"},{"location":"chart-developer/#kubernetes-dashboard","text":"If you use a microk8s installation of Kubernetes, then the procedure is slightly different. # Deploy the dashboard >> microk8s dashboard-proxy # Start the dashboard >> microk8s dashboard-proxy # Now access Dashboard at: https://localhost:10443","title":"Kubernetes Dashboard"},{"location":"chart-developer/#controller-logs","text":"The logs of the controller are accessible by the terminal on which the controller is running.","title":"Controller Logs"},{"location":"chart-developer/#grafana-dashboard-alerts","text":"Grafana is a multi-platform open source analytics and interactive visualization web application. To access it, use the format http://grafana.${INGRESS} where Ingress is the value you defined in step 3. For example, # Access Grafana via your browser http://grafana.platform.science-hangar.eu Optionally, validate that everything works. # Deploy a hello world >> kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4 deployment.apps/hello-node created # Verify that a hell-node deployment exists >> kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-node 1/1 1 1 36s # Delete the deployment >> kubectl delete deployments hello-node deployment.apps \"hello-node\" deleted This step will install the Frisbee CRDs and all the necessary tools. # Update Helm repo >> helm repo add frisbee https://carv-ics-forth.github.io/frisbee/charts # Install the platform with local ingress >> helm upgrade --install --wait my-frisbee frisbee/platform KUBECONFIG=\"/home/fnikol/.kube/config.evolve.admin\" make run ahelm upgrade --install --wait my-frisbee ./charts/platform/ --debug --set operator.enabled=false -f ./charts/platform/values-baremetal.yaml ehelm upgrade --install --wait my-system ./charts/system --debug Cool hacks: https://github.com/antrea-io/antrea/tree/main/hack","title":"Grafana Dashboard &amp; Alerts"},{"location":"chart-developer/#helm","text":"Create a public Helm chart repository with GitHub Pages What is Helm and a Helm chart repository Helm, as per official claim, is \u201cThe package manager for Kubernetes\u201d. Indeed Helm really helps handling application deployments on Kubernetes, not only because it simplifies the application release phase but also because Helm makes possible to easily manage variables in the Kubernetes manifest YAML files. Once the charts are ready and you need to share them, the easiest way to do so is by uploading them to a chart repository. A Helm chart repository is where we host and share Helm packages and any HTTP server will do. Unluckily Helm does not include natively a tool for uploading charts to a remote chart repository server (Helm can serve the local repository via $ helm serve though). We\u2019ll take advantage of GitHub Pages for the purpose to share our charts. What is GitHub Pages GitHub Pages is a static site hosting service provided to you by GitHub, designed to host your pages directly from a GitHub repository. GitHub Pages is a perfect match for our purpose, since everything we need is to host a single index.yaml file along with a bunch of .tgz files. Why not hosting all this stuff in your own web server? A managed service helps to reduce your operational overhead and risk (think to monitoring, patch management, security, backup services\u2026) so you can focus on code and in what makes your business relevant for your customers. Useful links: https://medium.com/@mattiaperi/create-a-public-helm-chart-repository-with-github-pages-49b180dbb417 https://github.com/technosophos/tscharts A Test Plan documents the strategy that will be used to verify a specific test for a software product. The plan typically contains a detailed understanding of the eventual workflow that models the deployment scenario for a software product, the test strategy, the resources required to perform testing, and the Key Performance Indicators required to ensure that a product or system meets its design specifications and other requirements. By codifying the test plan in a YAML-based syntax, Frisbee carriers three main benefits to teams: Help people outside the test team such as developers, business managers, customers understand the details of testing. Test Plan guides our thinking. It is like a rule book, which needs to be followed. Important aspects like test estimation, test scope, Test Strategy are documented in Test Plan, so it can be reviewed by Management Team and re-used for other projects. A test plan may include a strategy for one or more of the following: Baseline: to be performed during the development or approval stages of the product, typically on a small sample of units. Stress: to be performed during preparation or assembly of the product, in an ongoing manner for purposes of performance verification and quality control.","title":"HELM"},{"location":"chart-developer/#baseline","text":"A baseline is a fixed point of reference that is used for comparison purposes .","title":"Baseline"},{"location":"chart-developer/#ycsb","text":"Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs . It is often used to compare relative performance of NoSQL database management systems. All six workloads have a data set which is similar. Workloads D and E insert records during the test run. Thus, to keep the database size consistent, we apply the following sequence: Bootstrap the database. Load the database, using workload A's parameter file (workloads/workloada) and the \"-load\" switch to the client. Run workload A (using workloads/workloada and \"-t\") for a variety of throughputs. Run workload B (using workloads/workloadb and \"-t\") for a variety of throughputs. Run workload C (using workloads/workloadc and \"-t\") for a variety of throughputs. Run workload F (using workloads/workloadf and \"-t\") for a variety of throughputs. Run workload D (using workloads/workloadd and \"-t\") for a variety of throughputs. This workload inserts records, increasing the size of the database. Delete the data in the database. Otherwise, the remaining data of the cluster might affect the results of the following workload. For the deletion, instead of destroying the cluster, we destroy and recreate the cluster. Reload the database, using workload E's parameter file (workloads/workloade) and the \"-load switch to the client. Run workload E (using workloads/workloadd and \"-t\") for a variety of throughputs. This workload inserts records, increasing the size of the database. In general, these steps remain the same for the various databases. The difference is how we bootstrap each database.","title":"YCSB"},{"location":"chart-developer/#fio","text":"To benchmark persistent disk performance, use FIO instead of other disk benchmarking tools such as dd . Fio spawns a number of threads or processes doing a particular type of I/O action as specified by the user.","title":"FIO"},{"location":"chart-developer/#stress","text":"","title":"Stress"},{"location":"chart-developer/#scaleout","text":"","title":"Scaleout"},{"location":"chart-developer/#elasticity","text":"","title":"Elasticity"},{"location":"chart-developer/#chaos","text":"","title":"Chaos"},{"location":"chart-developer/#guide-for-the-frisbee-plan-developers","text":"Spurious Alert may be risen if the expr evaluation frequency is less than the scheduled interval. In this case, Grafana faces an idle period, and raises a NoData Alert. The controller ignores such messages.","title":"Guide for the Frisbee Plan Developers"},{"location":"chart-developer/#periodically-kill-some-nodes","text":"action: Cascade name: killer depends: { running: [ clients ] } cascade: templateRef: system.chaos.pod.kill instances: 3 inputs: { target: .cluster.clients.one } This can be wrong because Frisbee selects a single client -- and will be used 3 times, without error. Instead, we must use as many inputs as the number of instances -- or omit instances. In general, because when you use one input for multiple instances. Macros select only Running objects","title":"Periodically kill some nodes."},{"location":"chart-developer/#do-not-set-dependencies-on-cascades","text":"This is because Kills are always running -- therefore cascades that involve kill actions are always running","title":"Do not set dependencies on cascades"},{"location":"code-developer/","text":"Guide for the Frisbee Platform Developers Change the Code Make a new release Step 1: Change the version in the VERSION file Step 2: Update the controller's container Step 3: Update the Github repo Step 4: Create GitHub release Step 5: Validate release Change the Code # Fetch Frisbee >> git clone git@github.com:CARV-ICS-FORTH/frisbee.git >> cd frisbee There are two ways to run a Frisbee controller. As Go program outside a cluster. As a container inside a Kubernetes cluster. By default, Frisbee prefers the second way. However, when debugging a few feature it is impractical to have to create and deploy the container all the time. In such cases, it is preferable to run the controller outside the cluster, as shown next. Step 1: Firstly, we have to inform the Helm chart responsible for handling the platform deployment to not include the controller. # Remove the containerized controller from a running deployment >> helm upgrade --install my-frisbee charts/platform/ --set operator.enabled=false \\ --set global.ingress=platform.science-hangar.eu Notice we the flag global.ingress . Set it accordingly to the Ingress in your cluster. If you run Frisbee on a local cluster, simply omit the flag. Step 2: On the second step, we need to run the controller as a standalone binary. # Run Frisbee controller outside a cluster (from Frisbee directory) >> make run This may take a while as it has to take download the Go dependencies and compile the binary. If everything went fine, you should see something like the following. Tip: Beware the different between charts/platform and frisbee/platform . Given that we are under the Frisbee directory, the charts/platform points to the local copy of the chart, which is the one we want to modify. Otherwise, if frisbee/platform is used, Helm will use the released version of the repo. Make a new release Step 1: Change the version in the VERSION file Step 2: Update the controller's container If you have changes anything outside the charts folder, then you have probably modified some of the controller. In this case, you have to rebuild the container and push it to the registry. make docker-build IMG=\"yourImage\" Otherwise, you can use which will rebuild and automatically publish the container into the public Docker Hun registry, under the name icsforth / frisbee-operator >> make docker push Step 3: Update the Github repo We have also automated the way to publish new releases in Github. >> make release This will commit and tag all the changes in the repo. However, it will push changes to the Github. You have to do it manually using >> git push --set-upstream origin && git push --tags Step 4: Create GitHub release Go to GitHub and create a pull request Merge pull request Delete branch Go to GitHub Tags and create a new release for the latest tag The previous must have triggered some GitHub Actions. Go to https://github.com/CARV-ICS-FORTH/frisbee/actions and check that is everything is successful. Then go to the tags and create a new release for the latest tag. Step 5: Validate release The previous step have triggered the appropriate action for creating new charts. To confirm that they have been actually created, you can use the helm tool to search the repo. >> helm repo update >> helm search repo frisbee Then, you see that the latest Chart version is equal to the latest git tag , and in accordance to the VERSION file.","title":"Guide for the Frisbee Platform Developers"},{"location":"code-developer/#guide-for-the-frisbee-platform-developers","text":"Change the Code Make a new release Step 1: Change the version in the VERSION file Step 2: Update the controller's container Step 3: Update the Github repo Step 4: Create GitHub release Step 5: Validate release","title":"Guide for the Frisbee Platform Developers"},{"location":"code-developer/#change-the-code","text":"# Fetch Frisbee >> git clone git@github.com:CARV-ICS-FORTH/frisbee.git >> cd frisbee There are two ways to run a Frisbee controller. As Go program outside a cluster. As a container inside a Kubernetes cluster. By default, Frisbee prefers the second way. However, when debugging a few feature it is impractical to have to create and deploy the container all the time. In such cases, it is preferable to run the controller outside the cluster, as shown next. Step 1: Firstly, we have to inform the Helm chart responsible for handling the platform deployment to not include the controller. # Remove the containerized controller from a running deployment >> helm upgrade --install my-frisbee charts/platform/ --set operator.enabled=false \\ --set global.ingress=platform.science-hangar.eu Notice we the flag global.ingress . Set it accordingly to the Ingress in your cluster. If you run Frisbee on a local cluster, simply omit the flag. Step 2: On the second step, we need to run the controller as a standalone binary. # Run Frisbee controller outside a cluster (from Frisbee directory) >> make run This may take a while as it has to take download the Go dependencies and compile the binary. If everything went fine, you should see something like the following. Tip: Beware the different between charts/platform and frisbee/platform . Given that we are under the Frisbee directory, the charts/platform points to the local copy of the chart, which is the one we want to modify. Otherwise, if frisbee/platform is used, Helm will use the released version of the repo.","title":"Change the Code"},{"location":"code-developer/#make-a-new-release","text":"","title":"Make a new release"},{"location":"code-developer/#step-1-change-the-version-in-the-version-file","text":"","title":"Step 1: Change the version in the VERSION file"},{"location":"code-developer/#step-2-update-the-controllers-container","text":"If you have changes anything outside the charts folder, then you have probably modified some of the controller. In this case, you have to rebuild the container and push it to the registry. make docker-build IMG=\"yourImage\" Otherwise, you can use which will rebuild and automatically publish the container into the public Docker Hun registry, under the name icsforth / frisbee-operator >> make docker push","title":"Step 2: Update the controller's container"},{"location":"code-developer/#step-3-update-the-github-repo","text":"We have also automated the way to publish new releases in Github. >> make release This will commit and tag all the changes in the repo. However, it will push changes to the Github. You have to do it manually using >> git push --set-upstream origin && git push --tags","title":"Step 3: Update the Github repo"},{"location":"code-developer/#step-4-create-github-release","text":"Go to GitHub and create a pull request Merge pull request Delete branch Go to GitHub Tags and create a new release for the latest tag The previous must have triggered some GitHub Actions. Go to https://github.com/CARV-ICS-FORTH/frisbee/actions and check that is everything is successful. Then go to the tags and create a new release for the latest tag.","title":"Step 4: Create  GitHub release"},{"location":"code-developer/#step-5-validate-release","text":"The previous step have triggered the appropriate action for creating new charts. To confirm that they have been actually created, you can use the helm tool to search the repo. >> helm repo update >> helm search repo frisbee Then, you see that the latest Chart version is equal to the latest git tag , and in accordance to the VERSION file.","title":"Step 5: Validate release"},{"location":"faq/","text":"This is a WIP FAQ for Frisbee Q: The service seems fine, but I get a Failed message. Q: I changed some templates, but the changes does not seem to affect the Test Plan. Q: My experiment was running perfectly a few hours ago. Now, nothing works. Q: All I see in Grafana is a dot. There are no lines Q: My plots in Grafana are not in line. The times are different Q: I see dead plots ... on Grafana Q: Missing chart directory. Q: Pods don't have internet access Q: Ingress does not work Question: What is Frisbee ? Answer: Frisbee is a test-suite for Kubernetes. Q: The service seems fine, but I get a Failed message. A: The service run in a Pod that may host multiple containers. The application contrainer, the telemetry container, and so on. Given that, if the application seems fine, it is perhaps one of the sidecar containers that has failed. Q: I changed some templates, but the changes does not seem to affect the Test Plan. A: The changes are local and must be posted to the Kubernetes API. To update all templates within a chart use: >> helm upgrade --install --wait my-example ./charts/example --debug Q: My experiment was running perfectly a few hours ago. Now, nothing works. A: A possible explanation is that you do not specify the container version. If so, the latest version is retrieved for each run. And if there are incompatibilities between version, these incompatibilities will be reflected to your experiment. Q: All I see in Grafana is a dot. There are no lines A: This is likely to happen when the duration of the experiment is too short. In general, we use 1/4 resolution in order to make Grafana plots more readable. If you wish for a greater granularity, you can edit the chart and change resolution to 1/1. Q: My plots in Grafana are not in line. The times are different A: This is likely to happen if you have a change the resolution of one graph, without changing the other. Go and set the same resolution everywhere. Q: I see dead plots ... on Grafana A: Kubernetes v.1.22 drops support of cgroups v1 in favor of cgroup v2 API. If you run Kubernetes 1.22 make sure that you have set systemd.unified_cgroup_hierarchy=1 in the grub configuration. https://rootlesscontaine.rs/getting-started/common/cgroup2/ Q: Missing chart directory. A: It is possible to get helm.go:81: [debug] found in Chart.yaml, but missing in charts/ directory: when trying to run an experiment. This may happen if the dependencies of a chart (aka subcharts) are not installed. To fixed it, simply update the chart. for the chart platform (aka Frisbee), do the following >> helm dependency update charts/platform/ Q: Pods don't have internet access A: This may happen either because you have not enabled DNS on microk8s ( microk8s enable dns ) or because your firewall is blocking DNS traffic ( sudo ufw allow out to any port 53 ). If non of the above work, retry to reboot you machine ! https://github.com/canonical/microk8s/issues/1484 https://stackoverflow.com/questions/62664701/resolving-external-domains-from-within-pods-does-not-work Disable your firewall DNS issues will manifest as a [UFW BLOCK]... entries on the dmesg . If you see that, disable your firewall via sudo ufw disable Q: Ingress does not work A: Ingress issues will appear as 404 HTTP codes when trying to access the HTTP page. If so, run kubectl describe ingress | grep \"dashboard:\" and access the HTTP page via ip:port scheme (e.g, 10.1.128.37:8443) If the page is accessible then restart your machine and retry.","title":"Faq"},{"location":"faq/#this-is-a-wip-faq-for-frisbee","text":"Q: The service seems fine, but I get a Failed message. Q: I changed some templates, but the changes does not seem to affect the Test Plan. Q: My experiment was running perfectly a few hours ago. Now, nothing works. Q: All I see in Grafana is a dot. There are no lines Q: My plots in Grafana are not in line. The times are different Q: I see dead plots ... on Grafana Q: Missing chart directory. Q: Pods don't have internet access Q: Ingress does not work Question: What is Frisbee ? Answer: Frisbee is a test-suite for Kubernetes.","title":"This is a WIP FAQ for Frisbee"},{"location":"faq/#_1","text":"","title":""},{"location":"faq/#q-the-service-seems-fine-but-i-get-a-failed-message","text":"A: The service run in a Pod that may host multiple containers. The application contrainer, the telemetry container, and so on. Given that, if the application seems fine, it is perhaps one of the sidecar containers that has failed.","title":"Q: The service seems fine, but I get a Failed message."},{"location":"faq/#_2","text":"","title":""},{"location":"faq/#q-i-changed-some-templates-but-the-changes-does-not-seem-to-affect-the-test-plan","text":"A: The changes are local and must be posted to the Kubernetes API. To update all templates within a chart use: >> helm upgrade --install --wait my-example ./charts/example --debug","title":"Q:  I changed some templates, but the changes does not seem to affect the Test Plan."},{"location":"faq/#_3","text":"","title":""},{"location":"faq/#q-my-experiment-was-running-perfectly-a-few-hours-ago-now-nothing-works","text":"A: A possible explanation is that you do not specify the container version. If so, the latest version is retrieved for each run. And if there are incompatibilities between version, these incompatibilities will be reflected to your experiment.","title":"Q: My experiment was running perfectly a few hours ago. Now, nothing works."},{"location":"faq/#_4","text":"","title":""},{"location":"faq/#q-all-i-see-in-grafana-is-a-dot-there-are-no-lines","text":"A: This is likely to happen when the duration of the experiment is too short. In general, we use 1/4 resolution in order to make Grafana plots more readable. If you wish for a greater granularity, you can edit the chart and change resolution to 1/1.","title":"Q: All I see in Grafana is a dot. There are no lines"},{"location":"faq/#_5","text":"","title":""},{"location":"faq/#q-my-plots-in-grafana-are-not-in-line-the-times-are-different","text":"A: This is likely to happen if you have a change the resolution of one graph, without changing the other. Go and set the same resolution everywhere.","title":"Q: My plots in Grafana are not in line. The times are different"},{"location":"faq/#_6","text":"","title":""},{"location":"faq/#q-i-see-dead-plots-on-grafana","text":"A: Kubernetes v.1.22 drops support of cgroups v1 in favor of cgroup v2 API. If you run Kubernetes 1.22 make sure that you have set systemd.unified_cgroup_hierarchy=1 in the grub configuration. https://rootlesscontaine.rs/getting-started/common/cgroup2/","title":"Q: I see dead plots ... on Grafana"},{"location":"faq/#_7","text":"","title":""},{"location":"faq/#q-missing-chart-directory","text":"A: It is possible to get helm.go:81: [debug] found in Chart.yaml, but missing in charts/ directory: when trying to run an experiment. This may happen if the dependencies of a chart (aka subcharts) are not installed. To fixed it, simply update the chart. for the chart platform (aka Frisbee), do the following >> helm dependency update charts/platform/","title":"Q: Missing chart directory."},{"location":"faq/#_8","text":"","title":""},{"location":"faq/#q-pods-dont-have-internet-access","text":"A: This may happen either because you have not enabled DNS on microk8s ( microk8s enable dns ) or because your firewall is blocking DNS traffic ( sudo ufw allow out to any port 53 ). If non of the above work, retry to reboot you machine ! https://github.com/canonical/microk8s/issues/1484 https://stackoverflow.com/questions/62664701/resolving-external-domains-from-within-pods-does-not-work Disable your firewall DNS issues will manifest as a [UFW BLOCK]... entries on the dmesg . If you see that, disable your firewall via sudo ufw disable","title":"Q: Pods don't have internet access"},{"location":"faq/#_9","text":"","title":""},{"location":"faq/#q-ingress-does-not-work","text":"A: Ingress issues will appear as 404 HTTP codes when trying to access the HTTP page. If so, run kubectl describe ingress | grep \"dashboard:\" and access the HTTP page via ip:port scheme (e.g, 10.1.128.37:8443) If the page is accessible then restart your machine and retry.","title":"Q: Ingress does not work"},{"location":"tutorial/","text":"Tutorial Install Dependencies 1. Kubernetes and & Helm 2. Frisbee platform Testing a System 1. Deploy the system templates 2. Deploy the System Under Testing (SUT) 3. Verify the Deployment 4. Run a Scenario 5. Exploratory Testing (Observe the Progress) 6. Automated Testing (Pass/Fail) 7. Delete a Test 8. Parallel Tests Remove Frisbee 1. Delete all the namespaces you created 2. Remove Frisbee 3. Remove Frisbee CRDS This tutorial will guide you through deploying and running Frisbee on a local Kubernetes installation. Install Dependencies 1. Kubernetes and & Helm Microk8s is the simplest production-grade conformant K8s. It runs entirely on your workstation or edge device. Helm is a package manager for Kubernetes. Helm uses a packaging format called charts . # Install microk8s v.1.24 >> sudo snap install microk8s --classic --channel=1.24/stable # Use microk8s config as the default kubernetes config >> microk8s config > ~/.kube/config # Start microk8s >> microk8s start # Enable Dependencies >> microk8s enable dns ingress helm3 # Create aliases >> sudo snap alias microk8s.kubectl kubectl >> sudo snap alias microk8s.helm3 helm 2. Frisbee platform Although Frisbee can be installed directly from a Helm repository, for demonstration purposes we favor the git-based method. # Download the source code >> git clone git@github.com:CARV-ICS-FORTH/frisbee.git # Move to the Frisbee project >> cd frisbee # Have a look at the installation configuration >> less charts/platform/values.yaml # Make sure that the dir \"/mnt/local\" exists. The error will not appear until the execution of the test. >> mkdir /tmp/frisbee # Deploy the platform on the default namespace >> helm upgrade --install --wait my-frisbee ./charts/platform/ --debug -n default Testing a System 1. Deploy the system templates Firstly, we need to create a dedicated namespace for the test. The different namespaces provide isolation and allow us to run multiple tests in parallel. We combine the creation of the namespace and the installation of system templates (e.g, telemetry, chaos) in one command. >> helm upgrade --install --wait my-system ./charts/system --debug -n mytest --create-namespace 2. Deploy the System Under Testing (SUT) As a SUT we will use the CockroachDB . The commands are to be executed from the Frisbee directory. # Install Cockroach servers >> helm upgrade --install --wait my-cockroach ./charts/cockroachdb --debug -n mytest # Install YCSB for creating workload >> helm upgrade --install --wait my-ycsb ./charts/ycsb --debug -n mytest 3. Verify the Deployment Then you can verify that all the packages are successfully installed >> helm list NAME NAMESPACE REVISION UPDATED STATUS CHART my-frisbee default 1 2022-06-10 20:37:26.298297945 +0300 EEST deployed platform-0.0.0 >> helm list -n mytest NAME NAMESPACE REVISION UPDATED STATUS CHART my-cockroach mytest 1 2022-06-10 20:40:29.741398162 +0300 EEST deployed cockroachdb-0.0.0 my-system mytest 1 2022-06-10 20:40:19.981077906 +0300 EEST deployed defaults-0.0.0 my-ycsb mytest 1 2022-06-10 20:40:36.97639544 +0300 EEST deployed ycsb-0.0.0 4. Run a Scenario You now select which scenario you wish to run. >> ls -1a ./charts/cockroachdb/examples/ ... 10.bitrot.yml 11.network.yml 12.bitrot-logs.yml 1.baseline-single.yml 2.baseline-cluster-deterministic.yml 3.baseline-cluster-deterministic-outoforder.yml 4.baseline-cluster-nondeterministic.yml 5.scaleup-scheduled.yml 6.scaleup-conditional.yml 7.scaledown-delete.yml 8.scaledown-stop.yml 9.scaledown-kill.yml Let's run a bitrot scenario. >> kubectl -f ./charts/cockroachdb/examples/10.bitrot.yml apply -n mytest testplan.frisbee.io/cockroach-bitrot created 5. Exploratory Testing (Observe the Progress) Frisbee provides 3 methods for observing the progress of a test. Event-based : Consumes information from the Kubernetes API Dashboard : is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. Chaos Dashboard : is a one-step web UI for managing, designing, and monitoring chaos experiments on Chaos Mesh . It will ask for a token. You can get it from the config via grep token ~/.kube/config . Metrics-based: Consumes information from distributed performance metrics. Prometheus Grafana Log-based: Consumes information from distributed logs. Logviewer (admin/admin) You may notice that it takes long time for the experiment to start . This is due to preparing the NFS volume for collecting the logs from the various services. Also note that the lifecycle of the volume is bind to that of the test. If the test is deleted, the volume will be garbage collected automatically. 6. Automated Testing (Pass/Fail) The above tools are for understanding the behavior of a system, but do not help with test automation. Besides the visual information, we need something that can be used in external scripts. We will use kubectl since is the most common CLI interface between Kubernetes API and third-party applications. Firstly, let's inspect the test plan. >> kubectl describe testplan.frisbee.io/cockroach-bitrot -n mytest ... Status: Conditions: Last Transition Time: 2022-06-11T18:16:37Z Message: failed jobs: [run-workload] Reason: JobHasFailed Status: True Type: UnexpectedTermination Executed Actions: Bitrot: Boot: Import - Workload: Masters: Run - Workload: Grafana Endpoint: grafana-mytest.localhost Message: failed jobs: [run-workload] Phase: Failed Prometheus Endpoint: prometheus-mytest.localhost Reason: JobHasFailed We are interested in the Phase and Conditions fields that provides information about the present status of a test. The Phase describes the lifecycle of a Test. Phase Description \"\" The request is not yet accepted by the controller Pending The request has been accepted by the Kubernetes cluster, but one of the child jobs has not been created. This includes the time waiting for logical dependencies, Ports discovery, data rewiring, and placement of Pods. Running All the child jobs have been created, and at least one job is still running. Success All jobs have voluntarily exited. Failed At least one job of the CR has terminated in a failure (exited with a non-zero exit code or was stopped by the system). The Phase is a top-level description calculated based on some Conditions . The Conditions describe the various stages the Test has been through. Condition Description Initialized The workflow has been initialized AllJobsAreScheduled All jobs have been successfully scheduled. AllJobsAreCompleted All jobs have been successfully completed. UnexpectedTermination At least job that has been unexpectedly terminated. To avoid continuous inspection via polling, we use the wait function of kubectl . In the specific bitrot scenario, the test will pass only it has reached an UnexpectedTermination within 10 minutes of execution. >> kubectl wait --for=condition=UnexpectedTermination --timeout=10m testplan.frisbee.io/cockroach-bitrot -n mytest testplan.frisbee.io/cockroach-bitrot condition met Indeed, the condition is met, meaning that the test has failed. We can visually verify it from the Dashboard . To reduce the noise when debugging a failed test, Frisbee automatically deletes all the jobs, expect for the failed one (masters-1), and the telemetry stack (grafana/prometheus). If the condition is not met within the specified timeout, kubectl will exit with failure code (1) and the following error message: \"error: timed out waiting for the condition on testplans/cockroach-bitrot\" 7. Delete a Test The deletion is as simple as the creation of a test. >> kubectl -f /charts/cockroachdb/examples/10.bitrot.yml -n mytest delete --cascade=foreground testplan.frisbee.io \"cockroach-bitrot\" deleted The flag cascade=foreground will wait until the experiment is actually deleted. Without this flag, the deletion will happen in the background. Use this flag if you want to run sequential tests, without interference. 8. Parallel Tests For the time being, the safest to run multiple experiments is to run each test on a dedicated namespace . To do so, you have to repeat Step 1, replacing the -n .... flag with a different namespace. For example: Run bitrot >> helm upgrade --install --wait my-system ./charts/system --debug -n mytest --create-namespace >> helm upgrade --install --wait my-cockroach ./charts/cockroachdb --debug -n mytest >> helm upgrade --install --wait my-ycsb ./charts/ycsb --debug -n mytest >> kubectl -f ./charts/cockroachdb/examples/10.bitrot.yml apply -n mytest # go to http://grafana-mytest.localhost/d/crdb-console-runtime/crdb-console-runtime Run network failure >> helm upgrade --install --wait my-system ./charts/system --debug -n mytest2 --create-namespace >> helm upgrade --install --wait my-cockroach ./charts/cockroachdb --debug -n mytest2 >> helm upgrade --install --wait my-ycsb ./charts/ycsb --debug -n mytest2 >> kubectl -f ./charts/cockroachdb/examples/11.network.yml apply -n mytest2 # go to http://grafana-mytest2.localhost/d/crdb-console-runtime/crdb-console-runtime Notice that for every experiment, we start a new dedicated monitoring stack. Remove Frisbee 1. Delete all the namespaces you created By deleting the namespaces, you also delete all the installed components. >> kubectl delete namespace mytest mytest2 --wait Notice that it may take some time. 2. Remove Frisbee >> helm uninstall my-frisbee --debug -n default 3. Remove Frisbee CRDS Because Helm does not delete CRDs for secure reasons, we must do it manually. >> kubectl get crds | awk /frisbee.io/'{print $1}' | xargs -I{} kubectl delete crds {}","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"Install Dependencies 1. Kubernetes and & Helm 2. Frisbee platform Testing a System 1. Deploy the system templates 2. Deploy the System Under Testing (SUT) 3. Verify the Deployment 4. Run a Scenario 5. Exploratory Testing (Observe the Progress) 6. Automated Testing (Pass/Fail) 7. Delete a Test 8. Parallel Tests Remove Frisbee 1. Delete all the namespaces you created 2. Remove Frisbee 3. Remove Frisbee CRDS This tutorial will guide you through deploying and running Frisbee on a local Kubernetes installation.","title":"Tutorial"},{"location":"tutorial/#install-dependencies","text":"","title":"Install Dependencies"},{"location":"tutorial/#1-kubernetes-and-helm","text":"Microk8s is the simplest production-grade conformant K8s. It runs entirely on your workstation or edge device. Helm is a package manager for Kubernetes. Helm uses a packaging format called charts . # Install microk8s v.1.24 >> sudo snap install microk8s --classic --channel=1.24/stable # Use microk8s config as the default kubernetes config >> microk8s config > ~/.kube/config # Start microk8s >> microk8s start # Enable Dependencies >> microk8s enable dns ingress helm3 # Create aliases >> sudo snap alias microk8s.kubectl kubectl >> sudo snap alias microk8s.helm3 helm","title":"1. Kubernetes and &amp; Helm"},{"location":"tutorial/#2-frisbee-platform","text":"Although Frisbee can be installed directly from a Helm repository, for demonstration purposes we favor the git-based method. # Download the source code >> git clone git@github.com:CARV-ICS-FORTH/frisbee.git # Move to the Frisbee project >> cd frisbee # Have a look at the installation configuration >> less charts/platform/values.yaml # Make sure that the dir \"/mnt/local\" exists. The error will not appear until the execution of the test. >> mkdir /tmp/frisbee # Deploy the platform on the default namespace >> helm upgrade --install --wait my-frisbee ./charts/platform/ --debug -n default","title":"2. Frisbee platform"},{"location":"tutorial/#testing-a-system","text":"","title":"Testing a System"},{"location":"tutorial/#1-deploy-the-system-templates","text":"Firstly, we need to create a dedicated namespace for the test. The different namespaces provide isolation and allow us to run multiple tests in parallel. We combine the creation of the namespace and the installation of system templates (e.g, telemetry, chaos) in one command. >> helm upgrade --install --wait my-system ./charts/system --debug -n mytest --create-namespace","title":"1. Deploy the system templates"},{"location":"tutorial/#2-deploy-the-system-under-testing-sut","text":"As a SUT we will use the CockroachDB . The commands are to be executed from the Frisbee directory. # Install Cockroach servers >> helm upgrade --install --wait my-cockroach ./charts/cockroachdb --debug -n mytest # Install YCSB for creating workload >> helm upgrade --install --wait my-ycsb ./charts/ycsb --debug -n mytest","title":"2. Deploy the System Under Testing (SUT)"},{"location":"tutorial/#3-verify-the-deployment","text":"Then you can verify that all the packages are successfully installed >> helm list NAME NAMESPACE REVISION UPDATED STATUS CHART my-frisbee default 1 2022-06-10 20:37:26.298297945 +0300 EEST deployed platform-0.0.0 >> helm list -n mytest NAME NAMESPACE REVISION UPDATED STATUS CHART my-cockroach mytest 1 2022-06-10 20:40:29.741398162 +0300 EEST deployed cockroachdb-0.0.0 my-system mytest 1 2022-06-10 20:40:19.981077906 +0300 EEST deployed defaults-0.0.0 my-ycsb mytest 1 2022-06-10 20:40:36.97639544 +0300 EEST deployed ycsb-0.0.0","title":"3. Verify the Deployment"},{"location":"tutorial/#4-run-a-scenario","text":"You now select which scenario you wish to run. >> ls -1a ./charts/cockroachdb/examples/ ... 10.bitrot.yml 11.network.yml 12.bitrot-logs.yml 1.baseline-single.yml 2.baseline-cluster-deterministic.yml 3.baseline-cluster-deterministic-outoforder.yml 4.baseline-cluster-nondeterministic.yml 5.scaleup-scheduled.yml 6.scaleup-conditional.yml 7.scaledown-delete.yml 8.scaledown-stop.yml 9.scaledown-kill.yml Let's run a bitrot scenario. >> kubectl -f ./charts/cockroachdb/examples/10.bitrot.yml apply -n mytest testplan.frisbee.io/cockroach-bitrot created","title":"4. Run a Scenario"},{"location":"tutorial/#5-exploratory-testing-observe-the-progress","text":"Frisbee provides 3 methods for observing the progress of a test. Event-based : Consumes information from the Kubernetes API Dashboard : is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. Chaos Dashboard : is a one-step web UI for managing, designing, and monitoring chaos experiments on Chaos Mesh . It will ask for a token. You can get it from the config via grep token ~/.kube/config . Metrics-based: Consumes information from distributed performance metrics. Prometheus Grafana Log-based: Consumes information from distributed logs. Logviewer (admin/admin) You may notice that it takes long time for the experiment to start . This is due to preparing the NFS volume for collecting the logs from the various services. Also note that the lifecycle of the volume is bind to that of the test. If the test is deleted, the volume will be garbage collected automatically.","title":"5. Exploratory Testing (Observe the Progress)"},{"location":"tutorial/#6-automated-testing-passfail","text":"The above tools are for understanding the behavior of a system, but do not help with test automation. Besides the visual information, we need something that can be used in external scripts. We will use kubectl since is the most common CLI interface between Kubernetes API and third-party applications. Firstly, let's inspect the test plan. >> kubectl describe testplan.frisbee.io/cockroach-bitrot -n mytest ... Status: Conditions: Last Transition Time: 2022-06-11T18:16:37Z Message: failed jobs: [run-workload] Reason: JobHasFailed Status: True Type: UnexpectedTermination Executed Actions: Bitrot: Boot: Import - Workload: Masters: Run - Workload: Grafana Endpoint: grafana-mytest.localhost Message: failed jobs: [run-workload] Phase: Failed Prometheus Endpoint: prometheus-mytest.localhost Reason: JobHasFailed We are interested in the Phase and Conditions fields that provides information about the present status of a test. The Phase describes the lifecycle of a Test. Phase Description \"\" The request is not yet accepted by the controller Pending The request has been accepted by the Kubernetes cluster, but one of the child jobs has not been created. This includes the time waiting for logical dependencies, Ports discovery, data rewiring, and placement of Pods. Running All the child jobs have been created, and at least one job is still running. Success All jobs have voluntarily exited. Failed At least one job of the CR has terminated in a failure (exited with a non-zero exit code or was stopped by the system).","title":"6. Automated Testing (Pass/Fail)"},{"location":"tutorial/#_1","text":"The Phase is a top-level description calculated based on some Conditions . The Conditions describe the various stages the Test has been through. Condition Description Initialized The workflow has been initialized AllJobsAreScheduled All jobs have been successfully scheduled. AllJobsAreCompleted All jobs have been successfully completed. UnexpectedTermination At least job that has been unexpectedly terminated. To avoid continuous inspection via polling, we use the wait function of kubectl . In the specific bitrot scenario, the test will pass only it has reached an UnexpectedTermination within 10 minutes of execution. >> kubectl wait --for=condition=UnexpectedTermination --timeout=10m testplan.frisbee.io/cockroach-bitrot -n mytest testplan.frisbee.io/cockroach-bitrot condition met Indeed, the condition is met, meaning that the test has failed. We can visually verify it from the Dashboard . To reduce the noise when debugging a failed test, Frisbee automatically deletes all the jobs, expect for the failed one (masters-1), and the telemetry stack (grafana/prometheus). If the condition is not met within the specified timeout, kubectl will exit with failure code (1) and the following error message: \"error: timed out waiting for the condition on testplans/cockroach-bitrot\"","title":""},{"location":"tutorial/#7-delete-a-test","text":"The deletion is as simple as the creation of a test. >> kubectl -f /charts/cockroachdb/examples/10.bitrot.yml -n mytest delete --cascade=foreground testplan.frisbee.io \"cockroach-bitrot\" deleted The flag cascade=foreground will wait until the experiment is actually deleted. Without this flag, the deletion will happen in the background. Use this flag if you want to run sequential tests, without interference.","title":"7. Delete a Test"},{"location":"tutorial/#8-parallel-tests","text":"For the time being, the safest to run multiple experiments is to run each test on a dedicated namespace . To do so, you have to repeat Step 1, replacing the -n .... flag with a different namespace. For example: Run bitrot >> helm upgrade --install --wait my-system ./charts/system --debug -n mytest --create-namespace >> helm upgrade --install --wait my-cockroach ./charts/cockroachdb --debug -n mytest >> helm upgrade --install --wait my-ycsb ./charts/ycsb --debug -n mytest >> kubectl -f ./charts/cockroachdb/examples/10.bitrot.yml apply -n mytest # go to http://grafana-mytest.localhost/d/crdb-console-runtime/crdb-console-runtime Run network failure >> helm upgrade --install --wait my-system ./charts/system --debug -n mytest2 --create-namespace >> helm upgrade --install --wait my-cockroach ./charts/cockroachdb --debug -n mytest2 >> helm upgrade --install --wait my-ycsb ./charts/ycsb --debug -n mytest2 >> kubectl -f ./charts/cockroachdb/examples/11.network.yml apply -n mytest2 # go to http://grafana-mytest2.localhost/d/crdb-console-runtime/crdb-console-runtime Notice that for every experiment, we start a new dedicated monitoring stack.","title":"8. Parallel Tests"},{"location":"tutorial/#remove-frisbee","text":"","title":"Remove Frisbee"},{"location":"tutorial/#1-delete-all-the-namespaces-you-created","text":"By deleting the namespaces, you also delete all the installed components. >> kubectl delete namespace mytest mytest2 --wait Notice that it may take some time.","title":"1. Delete all the namespaces you created"},{"location":"tutorial/#2-remove-frisbee","text":">> helm uninstall my-frisbee --debug -n default","title":"2. Remove Frisbee"},{"location":"tutorial/#3-remove-frisbee-crds","text":"Because Helm does not delete CRDs for secure reasons, we must do it manually. >> kubectl get crds | awk /frisbee.io/'{print $1}' | xargs -I{} kubectl delete crds {}","title":"3. Remove Frisbee CRDS"}]}